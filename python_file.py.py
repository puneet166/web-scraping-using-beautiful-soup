# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_TWT9Za9AKgNz9mGTID6LzaY0KNzcYI

**Task 1-
extract Emails from the pages. **
"""

import re           # its for regular expression , we will use this for Email pattern matching. 
all_mails=[]          # it will use for storing emails.
def check_mail_idd(args):           # its function, which fetching emails from web pages.
        sou=args.text       # extract text of web page and store into sou varibale.
        listt = re.findall(r'[a-zA-Z0-9-_]{1,}@[a-zA-Z0-9-_]{1,}.[a-zA-Z]{1,}', sou)          # find pattern of this regex in text and extract . and store into listt variable.
        if (listt not in   all_mails):          # if mail not in all_mails list , then we will append that particular email id into all_mails list..
            all_mails.append(listt)         # append

import requests    # its use for give request to the server 
from bs4 import BeautifulSoup # its liberary that use for scraping


usefull_link=[]    # blank list , we will use this list for store urls.
#  here ask url from users. 
url =%sx read -p ''     # its google colab way to get the input from the users
url=url[0]      # extract 0 index string. and store is in url variable. we will use this variable for further processing.

try: # here we are putting try block for exception, if responce not come or user enter wrong url.  
   res= requests.get(url)       # request for this url, server will return responce and responce store in res object.[its all about https request responce] 
except requests.exceptions.RequestException as e: # if user enter wrong url and url not found exception handle over here.
        raise SystemExit("Error- No url found , please enter correct url")


# most of the time emails ids find on some others pages instead of home pages , like contact us pages, service page,about page and carrier pages. 


soup=BeautifulSoup(res.text,'html.parser')    # html content convert into html parser tree using beautifulsoup..

# now we will find all the links of the url page or current working page.


check_mail_idd(soup)      # here we are fetching email id , if emails id availabe on homepage and current working page.



links = [a.attrs.get('href') for a in soup.select('a[href]') ]       # here we are extracting all links which is availabe on the current working page, using list comprehensions

for i in links:        # extractng links one by one from links variable  and store in i variable for further processing  , its for loop.
    if("contact" in i or "Contact" in i )or("Career" in i or "career" in i)or('about' in i or "About" in i)or('Services' in i or 'services' in i):       # over here put some condition , if all these string availabe in i then 
    #i (links ) append in usefull_link list for further processing.

      usefull_link.append(i)        # append links in usefull_link which is type of list.

for i in usefull_link:          # now iterating usefull links one by one using for loop...
    if (i.startswith('http') or i.startswith('www') or i.startswith('WWW') or i.startswith('https')):         # here put condition , if links startswith www,https,http,WWW etc, 
      r=requests.get(i)           # with the matching result , again we will request to the server for that particular matching links.then server will responce and reponce will store in r variable , which is object of http. 
      soup1=BeautifulSoup(r.text,'html.parser')         # its same step like above 
      check_mail_idd(soup1)     # here we are calling , check_mail_idd ,method with soup1.
    else:               # if link not start with http,https,www,WWW then control will come over here.
      new_url=url+i               # here add the new url with the exist url and store in new_url variable.
      r=requests.get(new_url)             # its same like above and same like is in if statements.
      soup2=BeautifulSoup(r.text,'html.parser')           # same 
      check_mail_idd(soup2)               # same.
  
if(len(all_mails)==0):                          # if after searching emails , if email not found then this if will excuted.
    print('no email found')

for i in range(len(all_mails)): # iterating all the emails..
      print(all_mails[i])

