{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfautTcHEA-U",
        "colab_type": "text"
      },
      "source": [
        "**Task 1-\n",
        "extract Emails from the pages. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDXbLUjAEEgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re           # its for regular expression , we will use this for Email pattern matching. \n",
        "all_mails=[]          # it will use for storing emails.\n",
        "def check_mail_idd(args):           # its function, which fetching emails from web pages.\n",
        "        sou=args.text       # extract text of web page and store into sou varibale.\n",
        "        listt = re.findall(r'[a-zA-Z0-9-_]{1,}@[a-zA-Z0-9-_]{1,}.[a-zA-Z]{1,}', sou)          # find pattern of this regex in text and extract . and store into listt variable.\n",
        "        if (listt not in   all_mails):          # if mail not in all_mails list , then we will append that particular email id into all_mails list..\n",
        "            all_mails.append(listt)         # append\n",
        "            \n",
        "            \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdnkwpwwEIH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests    # its use for give request to the server \n",
        "from bs4 import BeautifulSoup # its liberary that use for scraping\n",
        "\n",
        "\n",
        "usefull_link=[]    # blank list , we will use this list for store urls.\n",
        "#  here ask url from users. \n",
        "url =%sx read -p ''     # its google colab way to get the input from the users\n",
        "url=url[0]      # extract 0 index string. and store is in url variable. we will use this variable for further processing.\n",
        "\n",
        "try: # here we are putting try block for exception, if responce not come or user enter wrong url.  \n",
        "   res= requests.get(url)       # request for this url, server will return responce and responce store in res object.[its all about https request responce] \n",
        "except requests.exceptions.RequestException as e: # if user enter wrong url and url not found exception handle over here.\n",
        "        raise SystemExit(\"Error- No url found , please enter correct url\")\n",
        "\n",
        "\n",
        "# most of the time emails ids find on some others pages instead of home pages , like contact us pages, service page,about page and carrier pages. \n",
        "\n",
        "\n",
        "soup=BeautifulSoup(res.text,'html.parser')    # html content convert into html parser tree using beautifulsoup..\n",
        "\n",
        "# now we will find all the links of the url page or current working page.\n",
        "\n",
        "\n",
        "check_mail_idd(soup)      # here we are fetching email id , if emails id availabe on homepage and current working page.\n",
        "\n",
        "\n",
        "\n",
        "links = [a.attrs.get('href') for a in soup.select('a[href]') ]       # here we are extracting all links which is availabe on the current working page, using list comprehensions\n",
        "\n",
        "for i in links:        # extractng links one by one from links variable  and store in i variable for further processing  , its for loop.\n",
        "    if(\"contact\" in i or \"Contact\" in i )or(\"Career\" in i or \"career\" in i)or('about' in i or \"About\" in i)or('Services' in i or 'services' in i):       # over here put some condition , if all these string availabe in i then \n",
        "    #i (links ) append in usefull_link list for further processing.\n",
        "\n",
        "      usefull_link.append(i)        # append links in usefull_link which is type of list.\n",
        "\n",
        "for i in usefull_link:          # now iterating usefull links one by one using for loop...\n",
        "    if (i.startswith('http') or i.startswith('www') or i.startswith('WWW') or i.startswith('https')):         # here put condition , if links startswith www,https,http,WWW etc, \n",
        "      r=requests.get(i)           # with the matching result , again we will request to the server for that particular matching links.then server will responce and reponce will store in r variable , which is object of http. \n",
        "      soup1=BeautifulSoup(r.text,'html.parser')         # its same step like above \n",
        "      check_mail_idd(soup1)     # here we are calling , check_mail_idd ,method with soup1.\n",
        "    else:               # if link not start with http,https,www,WWW then control will come over here.\n",
        "      new_url=url+i               # here add the new url with the exist url and store in new_url variable.\n",
        "      r=requests.get(new_url)             # its same like above and same like is in if statements.\n",
        "      soup2=BeautifulSoup(r.text,'html.parser')           # same \n",
        "      check_mail_idd(soup2)               # same.\n",
        "  \n",
        "if(len(all_mails)==0):                          # if after searching emails , if email not found then this if will excuted.\n",
        "    print('no email found')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbRxPcTHEPyK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5533af0b-f9d3-4d9c-f9f3-abacd3b41166"
      },
      "source": [
        "for i in range(len(all_mails)): # iterating all the emails..\n",
        "      print(all_mails[i])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "['Work@Home']\n",
            "['Work@Home', 'feedback@lifewire.com', 'feedback@lifewire.com', 'press@lifewire.com', 'sales@lifewire.com']\n",
            "['notification@jobvite.com', 'security@dotdash.com', 'advertise@dotdash.com', 'press@dotdash.com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsmafYUEEjkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}